DDG.duckbar.add_array([{"signal":"medium","templates":{"detail":"info_detail"},"duckbar_topic":"About","model":"FatheadArticle","meta":{"dev_milestone":"live","id":"wikipedia_fathead","designer":null,"topic":["productivity"],"js_callback_name":"wikipedia","repo":"fathead","production_state":"online","src_options":{"src_info":"","source_skip":"","skip_qr":"","is_wikipedia":1,"min_abstract_length":"20","directory":"","skip_image_name":0,"skip_abstract":0,"skip_abstract_paren":0,"skip_icon":0,"skip_end":"0","is_mediawiki":1,"is_fanon":0,"language":"en"},"created_date":null,"status":"live","tab":"About","src_id":1,"is_stackexchange":null,"blockgroup":null,"src_domain":"en.wikipedia.org","src_name":"Wikipedia","example_query":"nikola tesla","dev_date":null,"unsafe":0,"signal_from":"wikipedia_fathead","maintainer":{"github":"duckduckgo"},"developer":[{"url":"http://www.duckduckhack.com","type":"ddg","name":"DDG Team"}],"src_url":null,"live_date":null,"attribution":null,"description":"Wikipedia","producer":null,"name":"Wikipedia","perl_module":"DDG::Fathead::Wikipedia"},"data":{"Type":"A","Image":"","Abstract":"Gradient descent is a first-order iterative optimization algorithm for finding the local minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point. If, instead, one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent. Gradient descent was originally proposed by Cauchy in 1847. Gradient descent is also known as steepest descent. However, gradient descent should not be confused with the method of steepest descent for approximating integrals.","DefinitionURL":"","Answer":"","ImageHeight":0,"AnswerType":"","ImageWidth":0,"Definition":"","RelatedTopics":[{"Icon":{"Width":"","Height":"","URL":""},"FirstURL":"https://duckduckgo.com/Conjugate_gradient_method","Result":"<a href=\"https://duckduckgo.com/Conjugate_gradient_method\">Conjugate gradient method</a> - In mathematics, the conjugate gradient method is an algorithm for the numerical solution of particular systems of linear equations, namely those whose matrix is symmetric and positive-definite.","Text":"Conjugate gradient method - In mathematics, the conjugate gradient method is an algorithm for the numerical solution of particular systems of linear equations, namely those whose matrix is symmetric and positive-definite."},{"Text":"Stochastic gradient descent - Stochastic gradient descent is an iterative method for optimizing an objective function with suitable smoothness properties. It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient by an estimate thereof.","Result":"<a href=\"https://duckduckgo.com/Stochastic_gradient_descent\">Stochastic gradient descent</a> - Stochastic gradient descent is an iterative method for optimizing an objective function with suitable smoothness properties. It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient by an estimate thereof.","FirstURL":"https://duckduckgo.com/Stochastic_gradient_descent","Icon":{"Width":"","Height":"","URL":""}},{"FirstURL":"https://duckduckgo.com/Rprop","Icon":{"URL":"","Height":"","Width":""},"Text":"Rprop - Rprop, short for resilient backpropagation, is a learning heuristic for supervised learning in feedforward artificial neural networks. This is a first-order optimization algorithm. This algorithm was created by Martin Riedmiller and Heinrich Braun in 1992.","Result":"<a href=\"https://duckduckgo.com/Rprop\">Rprop</a> - Rprop, short for resilient backpropagation, is a learning heuristic for supervised learning in feedforward artificial neural networks. This is a first-order optimization algorithm. This algorithm was created by Martin Riedmiller and Heinrich Braun in 1992."},{"Text":"Delta rule - In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm.","Result":"<a href=\"https://duckduckgo.com/Delta_rule\">Delta rule</a> - In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm.","FirstURL":"https://duckduckgo.com/Delta_rule","Icon":{"URL":"","Height":"","Width":""}},{"Text":"Wolfe conditions - In the unconstrained minimization problem, the Wolfe conditions are a set of inequalities for performing inexact line search, especially in quasi-Newton methods, first published by Philip Wolfe in 1969.","Result":"<a href=\"https://duckduckgo.com/Wolfe_conditions\">Wolfe conditions</a> - In the unconstrained minimization problem, the Wolfe conditions are a set of inequalities for performing inexact line search, especially in quasi-Newton methods, first published by Philip Wolfe in 1969.","FirstURL":"https://duckduckgo.com/Wolfe_conditions","Icon":{"URL":"","Width":"","Height":""}},{"Icon":{"URL":"","Height":"","Width":""},"FirstURL":"https://duckduckgo.com/Preconditioner","Result":"<a href=\"https://duckduckgo.com/Preconditioner\">Preconditioning</a> - In mathematics, preconditioning is the application of a transformation, called the preconditioner, that conditions a given problem into a form that is more suitable for numerical solving methods. Preconditioning is typically related to reducing a condition number of the problem.","Text":"Preconditioning - In mathematics, preconditioning is the application of a transformation, called the preconditioner, that conditions a given problem into a form that is more suitable for numerical solving methods. Preconditioning is typically related to reducing a condition number of the problem."},{"Result":"<a href=\"https://duckduckgo.com/c/Gradient_methods\">Gradient methods</a>","Text":"Gradient methods","Icon":{"URL":"","Width":"","Height":""},"FirstURL":"https://duckduckgo.com/c/Gradient_methods"},{"Icon":{"URL":"","Height":"","Width":""},"FirstURL":"https://duckduckgo.com/c/First_order_methods","Result":"<a href=\"https://duckduckgo.com/c/First_order_methods\">First order methods</a>","Text":"First order methods"}],"Infobox":"","AbstractSource":"Wikipedia","AbstractText":"Gradient descent is a first-order iterative optimization algorithm for finding the local minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point. If, instead, one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent. Gradient descent was originally proposed by Cauchy in 1847. Gradient descent is also known as steepest descent. However, gradient descent should not be confused with the method of steepest descent for approximating integrals.","meta":{"dev_milestone":"live","id":"wikipedia_fathead","designer":null,"topic":["productivity"],"js_callback_name":"wikipedia","repo":"fathead","production_state":"online","src_options":{"src_info":"","source_skip":"","skip_qr":"","is_wikipedia":1,"min_abstract_length":"20","directory":"","skip_image_name":0,"skip_abstract":0,"skip_abstract_paren":0,"skip_icon":0,"skip_end":"0","is_mediawiki":1,"is_fanon":0,"language":"en"},"created_date":null,"status":"live","tab":"About","src_id":1,"is_stackexchange":null,"blockgroup":null,"src_domain":"en.wikipedia.org","src_name":"Wikipedia","example_query":"nikola tesla","dev_date":null,"unsafe":0,"signal_from":"wikipedia_fathead","maintainer":{"github":"duckduckgo"},"developer":[{"url":"http://www.duckduckhack.com","type":"ddg","name":"DDG Team"}],"src_url":null,"live_date":null,"attribution":null,"description":"Wikipedia","producer":null,"name":"Wikipedia","perl_module":"DDG::Fathead::Wikipedia"},"Heading":"Gradient descent","ImageIsLogo":0,"AbstractURL":"https://en.wikipedia.org/wiki/Gradient_descent","DefinitionSource":"","Results":[],"Redirect":"","Entity":""},"from":"nlp_fathead"}]);